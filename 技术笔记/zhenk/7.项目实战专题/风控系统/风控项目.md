# 网易考拉规则引擎平台架构设计与实践
![[image-20231109153148169.png]]

>主要用于反作弊欺诈领域的业务，例如 登录，注册，领券等
## 背景  

考拉安全部技术这块目前主要负责两块业务：一个是内审，主要是通过敏感日志管理平台搜集考拉所有后台系统的操作日志，数据导入到es后，结合storm进行实时计算，主要有行为查询、数据监控、事件追溯、风险大盘等功能；一个是业务风控，主要是下单、支付、优惠券、红包、签到等行为的风险控制，对抗的风险行为包括黄牛刷单、恶意占用库存、机器领券、撸羊毛等。这两块业务其实有一个共通点，就是有大量需要进行规则决策的场景，比如内审中需要进行实时监控，当同一个人在一天时间内的导出操作超过多少次后进行告警，当登录时不是常用地登录并且设备指纹不是该账号使用过的设备指纹时告警。而在业务风控中需要使用到规则决策的场景更多，由于涉及规则的保密性，这里就不展开了。总之，基于这个出发点，安全部决定开发出一个通用的规则引擎平台，来满足以上场景。

## 写在前面  

在给出整体架构前，想跟大家聊聊关于架构的一些想法。目前架构上的分层设计思想已经深入人心，大家都知道要分成controller,server,dao等，是因为我们刚接触到编码的时候，mvc的模型已经大行其道，早期的jsp里面包含大量业务代码逻辑的方式已经基本绝迹。但是这并不是一种面向对象的思考方式，而往往我们是以一种面向过程的思维去编程。举个简单例子，我们要实现一个网银账户之间转账的需求，往往会是下面这种实现方式：  

1. 设计一个账户交易服务接口AccountingService，设计一个服务方法transfer()，并提供一个具体实现类AccountingServiceImpl，所有账户交易业务的业务逻辑都置于该服务类中。
2. 提供一个AccountInfo和一个Account，前者是一个用于与展示层交换账户数据的账户数据传输对象，后者是一个账户实体（相当于一个EntityBean），这两个对象都是普通的JavaBean，具有相关属性和简单的get/set方法。
3. 然后在transfer方法中，首先获取A账户的余额，判断是否大于转账的金额，如果大于则扣减A账户的余额，并增加对应的金额到B账户。
  

这种设计在需求简单的情况下看上去没啥问题，但是当需求变得复杂后，会导致代码变得越来越难以维护，整个架构也会变的腐烂。比如现在需要增加账户的信用等级，不同等级的账户每笔转账的最大金额不同，那么我们就需要在service里面加上这个逻辑。后来又需要记录转账明细，我们又需要在service里面增加相应的代码逻辑。最后service代码会由于需求的不断变化变得越来越长，最终变成别人眼中的“祖传代码”。导致这个问题的根源，我认为就是我们使用的是一种面向过程的编程思想。那么如何去解决这种问题呢？主要还是思维方式上需要改变，我们需要一种真正的面向对象的思维方式。比如一个“人”，除了有id、姓名、性别这些属性外，还应该有“走路”、“吃饭”等这些行为，这些行为是天然属于“人”这个实体的，而我们定义的bean都是一种“失血模型”，只有get/set等简单方法，所有的行为逻辑全部上升到了service层，这就导致了service层过于臃肿，并且很难复用已有的逻辑，最后形成了各个service之间错综复杂的关联关系，在做服务拆分的时候，很难划清业务边界，导致服务化进程陷入泥潭。

  

对应上面的问题，我们可以在Account这个实体中加入本应该就属于这个实体的行为，比如借记、贷记、转账等。每一笔转账都对应着一笔交易明细，我们根据交易明细可以计算出账户的余额，这个是一个潜在的业务规则，这种业务规则都需要交由实体本身来维护。另外新增账户信用实体，提供账户单笔转账的最大金额计算逻辑。这样我们就把原本全部在service里面的逻辑划入到不同的负责相关职责的“领域对象”当中了，service的逻辑变得非常清楚明了，想实现A给B转账，直接获取A实体，然后调用A实体中的转账方法即可。service将不再关注转账的细节，只负责将相关的实体组织起来，完成复杂的业务逻辑处理。

  

上面的这种架构设计方式，其实就是一种典型的“领域驱动设计(DDD)”思想，在这里就不展开说明了（主要是自己理解的还不够深入，怕误导大家了）。DDD也是目前非常热门的一种架构设计思想了，它不能减少你的代码量，但是能使你的代码具有很高的内聚性，当你的项目变得越来越复杂时，能保持架构的稳定而不至于过快的腐烂掉，不了解的同学可以查看相关资料。要说明的是，没有一种架构设计是万能的、是能解决所有问题的，我们需要做的是吸收好的架构设计思维方式，真正架构落地时还是需要根据实际情况选择合适的架构。

  

## 整体架构设计  

上面说了些架构设计方面的想法，现在我们回到规则引擎平台本身。我们抽象出了四个分层，从上到下分别为：服务层、引擎层、计算层和存储层，整个逻辑层架构见下图：

![](https://nos.netease.com/cloud-website-bucket/20181023100347a180e927-34bd-427a-b58e-622d5181972f.png)  

  

- 服务层：服务层主要是对外提供服务的入口层，提供的服务包括数据分析、风险检测、业务决策等，所有的服务全部都是通过数据接入模块接入数据，具体后面讲
  
- 引擎层：引擎层是整个平台的核心，主要包括了执行规则的规则引擎、还原事件现场和聚合查询分析的查询引擎以及模型预测的模型引擎

- 计算层：计算层主要包括了指标计算模块和模型训练模块。指标会在规则引擎中配置规则时使用到，而模型训练则是为模型预测做准备

- 存储层：存储层包括了指标计算结果的存储、事件信息详情的存储以及模型样本、模型文件的存储 


在各个分层的逻辑架构划定后，我们就可以开始分析整个平台的业务功能模块。主要包括了事件接入模块、指标计算模块、规则引擎模块、运营中心模块，整个业务架构如下图：

![](https://nos.netease.com/cloud-website-bucket/201810231004018de9943c-8de0-46cd-bb0c-7e758a62f50e.png)  
### 1.事件接入中心

事件接入中心主要包括事件接入模块和数据管理模块。数据接入模块是整个规则引擎的数据流入口，所有的业务方都是通过这个模块接入到平台中来。提供了实时(dubbo)、准实时（kafka）和离线（hive）三种数据接入方式。数据管理模块主要是进行事件的元数据管理、标准化接入数据、补全必要的字段，如下图： 
![](https://nos.netease.com/cloud-website-bucket/201810231004121c018977-e1f5-45c6-968d-39ff3500f366.png)
### 2.指标计算模块
指标计算模块主要是进行指标计算。一个指标由主维度、从维度、时间窗口等组成，其中主维度至少有一个，从维度最多有一个。如下图： 

![](https://nos.netease.com/cloud-website-bucket/2018102310042476116e86-4f15-4960-9920-d8f76f0d4015.jpg)  

举个例子，若有这样一个指标：“最近10分钟，同一个账号在同一个商家的下单金额”，那么主维度就是下单账号+商家id，从维度就是订单金额。可以看到，这里的主维度相当于sql里面的group by，从维度相当于count，数值累加相当于sum。从关于指标计算，有几点说明下：


1. key的构成。我们的指标存储是用的redis，那么这里会涉及到一个key该如何构建的问题。我们目前的做法是：key=指标id+版本号+主维度值+时间间隔序号。

	- 指标id就是指标的唯一标示；
	- 版本号是指标对象的版本，每次更新完指标都会更新对应的版本号，这样可以让就的指标一次全部失效；
	- 主维度值是指当前事件对象中，主维度字段对应的值，比如一个下单事件，主维度是用户账号，那么这里就是对应的类似XXX@163.com，如果有多个主维度则需要全部组装上去；
	- 如果主维度的值出现中文，这样直接拼接在key里面会有问题，可以采用转义或者md5的方式进行。
	- 时间间隔序号是指当前时间减去指标最后更新时间，得到的差值再除以采样周期，得到一个序号。这么做主要是为了实现指标的滑动窗口计算，下面会讲


2. 滑动窗口计算。比如我们的指标是最近10分钟的同一用户的下单量，那么我们就需要实现一种类似的滑动窗口算法，以便任何时候都能拿到“最近10分钟”的数据。这里我们采用的是一种简单的算法：创建指标时，指定好采样次数。比如要获取“最近10分钟”的数据，采样次数设置成30次，这样我们会把每隔20秒的数据会放入一个key里面。每次一个下单事件过来时，计算出时间间隔序号（见第1点），然后组装好key之后看该key是否存在，存在则进行累计，否则往redis中添加该key。

3. 如何批量获取key。每次获取指标值时，我们都是先计算出需要的key集合（比如我要获取“单个账号最近10分钟的下单量”，我可能需要获取30个key，因为每个key的跨度是20s），然后获取到对应的value集合，再进行累加。而实际上我们只是需要累加后的值，这里可以通过redis+lua脚本进行优化，脚本里面直接根据key集合获取value后进行累加然后返回给客户端，这样就较少了每次响应的数据量。

4. 如何保证指标的计算结果不丢失？目前的指标是存储在redis里面的，后来会切到solo-ldb，ldb提供了持久化的存储引擎，可以保证数据不丢失。

### 3.规则引擎模块  

计划开始做规则引擎时进行过调研，发现很多类似的平台都会使用drools。而我们从一开始就放弃了drools而全部使用groovy脚本实现，主要是有以下几点考虑：  

- drools相对来说有点重，而且它的规则语言不管对于开发还是运营来说都有学习成本

- drools使用起来没有groovy脚本灵活。groovy可以和spring完美结合，并且可以自定义各种组件实现插件化开发。

- 当规则集变得复杂起来时，使用drools管理起来有点力不从心。   

当然还有另外一种方式是将drools和groovy结合起来，综合双方的优点，也是一种不错的选择，大家可以尝试一下。  

规则引擎模块是整个平台的核心，我们将整个模块分成了以下几个部分： 

![](https://nos.netease.com/cloud-website-bucket/20181023100506855d3855-69fb-48fb-95a5-c0c047fa9e1e.png)  

规则引擎在设计中也碰到了一些问题，这里给大家分享下一些心得：  

- 使用插件的方式加载各种组件到上下文中，极大的方便了功能开发的灵活性。

- 使用预加载的方式加载已有的规则，并将加载后的对象缓存起来，每次规则变更时重新load整条规则，极大的提升了引擎的执行效率

- 计数器引入AtomicLongFieldUpdater工具类，来减少计数器的内存消耗

- 灵活的上下文使用方式，方便定制规则执行的流程（规则执行顺序、同步异步执行、跳过某些规则、规则集短路等），灵活定义返回结果（可以返回整个上下文，可以返回每条规则的结果，也可以返回最后一条规则的结果），这些都可以通过设置上下文来实现。
 
- groovy的方法查找策略，默认是从metaClass里面查找，再从上下文里找，为了提升性能，我们重写了metaClass，修改了这个查询逻辑：先从上下文里找，再从metaClass里面找。


规则配置如下图所示：

![](https://nos.netease.com/cloud-website-bucket/20181023100521f9aa1be6-f746-49a4-bbf6-6661dcb0ad5c.jpg)  

## 未来规划  

后面规则引擎平台主要会围绕下面几点来做：


1. 指标存储计划从redis切换到hbase。目前的指标计算方式会导致缓存key的暴涨，获取一个指标值可能需要N个key来做累加，而换成hbase之后，一个指标就只需要一条记录来维护，使用hbase的列族来实现滑动窗口的计算。

2. 规则的灰度上线。当一条新规则创建后，如果不进行灰度的测试，直接上线是可能会带来灾难的。后面再规则上线流程中新增灰度上线环节，整个引擎会根据配置的灰度比例，复制一定的流量到灰度规则中，并对灰度规则的效果进行展示，达到预期效果并稳定后才能审批上线。

3. 事件接入的自动化。dubbo这块可以采用泛化调用，http接口需要统一调用标准，消息需要统一格式。有了统一的标准就可以实现事件自动接入而不需要修改代码上线，这样也可以保证整个引擎的稳定性。

4. 模型生命周期管理。目前模型这块都是通过在猛犸平台上提交jar包的方式，离线跑一个model出来，没有一个统一的平台去管控整个模型的生命周期。现在杭研已经有类似的平台了，后续需要考虑如何介入。

5. 数据展示优化。现在整个平台的数字化做的比较弱，没法形成数据驱动业务。而风控的运营往往是需要大量的数据去驱动规则的优化的，比如规则阈值的调试、规则命中率、风险大盘等都需要大量数据的支撑。

# Argis，携程风控系统
![[image-20231109153312900.png]]

**作者简介**

蒋一新，携程风险控制部技术专家，负责风控运营平台、数据分发、关系图谱、名单服务等多个风控子系统；风控引擎性能优化主要实施人员。  

为了应对日益严重的支付欺诈，携程在线风控系统2011年正式上线。现在，在线风控系统支撑了携程每日1亿+的风险事件实时处理和100亿+的准实时数据预处理；系统中运行的总规则数和总模型数分别达到了1万+和20+；风控的范围从单纯的支付风控扩展到了各种类型的业务风控（例如：恶意抢占资源、黄牛抢购、商家刷单）。  

下图是当前在线风控系统的整体技术架构图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAn0lvaBISUCS2DmeayzRJCGTutafbiaQnvEXwGq8sa38MbvzSyMqZW0mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
当前的系统结构是比较主流的风控系统结构，包含了决策引擎、Counter、名单库、用户画像、离线处理、离线分析和监控各主要模块。携程的在线风控系统发展到这个阶段一共经过了3次重大的改版。

## 一、最初创立

2011年上线，使用了.net开发的服务，数据库使用SQL Server，简要架构图如下：  

![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAndol0fsT9ZiaYPeAV94oCng8w0ab4L2YthEy7dZMqqpMaZ46f5WmURHw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)


这个时候的风控服务将所有在线决策功能整合在一个系统内实现，包括规则判断、名单库、流量计算；而这些逻辑都基于数据库实现。

**流量计算**：通过明细表执行SQL得到（例如：SELECT COUNT(DISTINCT orderId) FROMt1 WHERE …[2]）


**规则判断**：数据库记录大于、小于、等于等判断规则，接收到风险事件后获取流量值和规则进行比较，得到最终的风险判断。


**名单库判断**：数据库维护黑白名单信息（属性类型、属性值、判断依据等），程序判断风险事件中的值是否命中名单。


基于当时携程对风控的需求，系统以满足功能为主。在上线运行一段时间后，随着携程业务的增长，风控系统的流量不断增加，基于SQL的流量统计耗时严重制约了系统的响应时间，因此有了第一次的性能优化改版。
## 二、流量查询性能优化改版

由于这个时候的主要性能瓶颈在于数据库实现的流量查询，这次优化主要方向就是优化流量查询的实现：在原来单个数据库的基础上，采用分库分表的方式均摊压力，以达到更快的响应时间和更高的吞吐量。架构图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAnSQia0EsR6zUic4SFWd27pUGMkTB2gLNVOYWoQTlo8SGmscL5CY9QhXug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
优化之后，流量库和业务库分离，流量库使用多个数据库实例，使用hash的方式拆分流量明细，统计的时候使用SQL。由于压力被多个数据库实例分摊，使得系统的流量查询性能得到了较好的提升。


新版本上线后，携程的业务又对风控系统提出了更多的要求：

1.    更方便快捷的接入：除了支付风险，业务的风险也需要风控支持；
2.    更多的外部数据接入：用户信息、位置信息、UBT信息；
3.    更丰富的规则逻辑：支持任意变量的规则判断，支持更多的判断逻辑；
4.    更高的性能：流量10x的增长，响应时间不超过1秒；
5.    编程语言的更新：携程推动公司内.net转java。

然后，就有了奠定当前在线风控系统基础的重大改版

## 三、风控3.0（Aegis）

从这个版本开始，风控系统全面转向Java开发，同时将核心模块独立成服务，定义了各子系统的边界以及在整个系统中的定位和作用。相比之前功能性的应用，Aegis是一个平台化的风控系统。以下是简化的系统架构图：  

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAn1u9PIiaLgvjRiaerrhx61IpYKnIdDUSgLd470mb1KJfAlkjZgeDt9zjw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)
Aegis开始使用Drools脚本用于规则的编写，极大的提高了规则团队对突发事件的响应时间，紧急规则一般10分钟就能上线。

在结构上风控引擎分为同步引擎和异步引擎，同步引擎运行用于实时判断风险结果的规则和模型；异步引擎则负责验证规则/模型、数据分发、关键数据落地等逻辑。同步/异步引擎设计成无状态的，方便随时扩容。

Aegis在流量统计上自研发了Counter Server，这是一个定制化的类TSDB服务，任意精度任意时间窗口的查询控制在5ms，同时支持高并发查询，相较于SQL的实现提升了上千倍的性能。支撑了现在风控系统内个服务每日百亿次的查询。下图是其简要的结构说明：

![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAnVFiaWRyj6jSGrzlRMicA41u7yAdhqJItC8oMiaHdcIDYM8AelNhCWHD6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
在数据预处理层面独立出了风险画像和DataProxy两个服务。

**风险画像服务**为引擎提供实时的用户、订单的画像（用户等级、用户行为标签、订单资源标签等）作为规则和模型的输入变量；其数据来源是实时的引擎数据、准实时的MQ数据清洗服务、离线的数据导入三部分。

**DataProxy服务**包装了所有对外的接口和数据库的访问，并针对数据特性的不同配置了不同的缓存策略，保证99.9%的请求在10ms内获取到所需的数据。

此外还有以下几个主要的服务：

**名单库服务**，支持多个独立的名单库，优化了名单判断逻辑，使得单次查询（10个维度）的响应小于10ms。

**配置服务**，集中系统内各个应用需要配置的功能，提供中心化的配置服务让各个应用获取响应配置。

**事件处理平台**，用于处理引擎无法判断或需要人工干预的事件。

**性能监控服务**，监控系统内各服务的健康状态，提供预警和报警功能。

**业务监控服务**，监控规则模型运行情况、返回的风险结果、事件耗时等业务层面的数据，提供预警和报警功能。

Aegis系统上线后，新风控业务接入时间缩短到一周，在10x流量增加和执行更多更复杂的规则的情况下，平均耗时控制在300+ms，比上一版本提高了1倍多。

之后Aegis家族又增加了两个重要的子系统：Sessionizer和DeviceID。这两个服务属于准实时处理应用，但是都通过预热的方式为引擎提供了实时数据。

Sessionizer，归约用户的页面访问session为反应用户的操作的RiskSession，流程如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAnqWrNvUYLETs7nW6uwBc12GZgV6fsGRgTA9Ps6FibfSJOAuvicVHlJ8mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
其数据处理使用了自研的大数据处理系统Chloro，架构图如下：
![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAnuhtCZl0mxr20UX2F00oiahsL3N3bWfWEG9w2cUgJX8YpSIibXqxlQLpw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

DeviceID服务，用于指纹数据采集和指纹识别生成，从而判断设备的唯一性，同样也借助了Chloro进行数据处理，系统示意图如下：
![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAn4HgLuEW8lT5GQhgtdT8tGvqicMvT4horancpVUCsomeyWsQoSEzozfQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这两个服务为规则和模型以及人工处理提供了非常关键的判断数据，进一步提高了风险判断的准确性。

只此，Aegis系统的功能已比较完善，但在1年多的运行过程中发现随着流量和规则、模型量的持续增长，实时风控的响应时间出现缓慢下滑的趋势，超时（1秒）的量在千分之一上下，然后就有了之后持续的性能优化。

## 四、针对实时风控的性能优化

性能优化从1年前开始，可以分以下几个主要优化：  

**1、规则分布式并行执行**
![图片](https://mmbiz.qpic.cn/mmbiz_png/kEeDgfCVf1d7QB4NnFc1GlWu2bFa5GAnJAdQQIDkJibS85icEiaRmQLEXyKo8POf8jlZdWpcTZW2r6ob7R9UXicofg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

将单个事件需要执行的规则和模型拆分到同一逻辑组内多个服务器执行，最终合并数据。这个优化将平均耗时降到了200+ms

**2、脚本执行引擎切换Drools到Java**

使用模版屏蔽编写规则时drools的特殊语法，之后将脚本编译成Java class执行。规则的执行性能提升1倍，整个引擎的实时平均耗时降入100ms。

**3、开发Java的模型执行引擎**

已完成随机森林和逻辑回归算法的Java版，相较使用Python，提高了一个数量级的性能。

完成上述优化后，整个系统在短期内，只需要简单的增加服务器，就可以满足容量扩张。

以上就是Aegis系统的架构和演进过程，当然演进的过程还在持续，现阶段的目标是将平台化的系统继续发展，做到服务产品化。

**推荐阅读：**  

 - [如何基于Spark Streaming构建实时计算平台](http://mp.weixin.qq.com/s?__biz=MjM5MDI3MjA5MQ==&mid=2697266508&idx=1&sn=4e5d05581686c15c691997c22d761bd7&chksm=8376fa78b401736e051a33048cca4a99cb38fe6d50156f1d20b93dd3931717cf1127ebda8788&scene=21#wechat_redirect)     
 - [一个MySQL5.7分区表性能下降的案例分析](http://mp.weixin.qq.com/s?__biz=MjM5MDI3MjA5MQ==&mid=2697266477&idx=1&sn=2ee075b1a04753b034510b4c88934b22&chksm=8376fa19b401730f672ef0926c1e9d8e46b37304b9fb2f487a95e8403ce0019f45d5e43de70e&scene=21#wechat_redirect)
 - [大规模知识图谱的构建、推理及应用](http://mp.weixin.qq.com/s?__biz=MjM5MDI3MjA5MQ==&mid=2697266451&idx=1&sn=264e01bf70c410ee5cee9b3d95a08a15&chksm=8376fa27b401733198c913c9a2be6a6622394a58864d698bc57ec2f6ee7666693249a626c8e7&scene=21#wechat_redirect)     
 - [携程运维工作流平台的演进之路](http://mp.weixin.qq.com/s?__biz=MjM5MDI3MjA5MQ==&mid=2697266427&idx=1&sn=5c1136e22540289a73535b5936ff265a&chksm=8376fbcfb40172d9506cc530e6d986f7b9dcf175452c982dcbeacfba1c0922c3458aed41af96&scene=21#wechat_redirect)   
 - [携程机票大数据架构最佳实践](http://mp.weixin.qq.com/s?__biz=MjM5MDI3MjA5MQ==&mid=2697266385&idx=1&sn=3c6642f57d322cc2873fa5b4730c8b64&chksm=8376fbe5b40172f3b8aacc1e868c6e189bec8a189b0cd53b300af5d7ea807f9afd43a62dc8d4&scene=21#wechat_redirect)